\documentclass[a4paper, 14pt]{extarticle}
\usepackage{geometry}

\usepackage{cmap} % Улучшенный поиск русских слов в полученном pdf-файле
\usepackage{mathtext} % русские буквы в формулах
\defaulthyphenchar=127 % Если стоит до fontenc, то переносы не впишутся в выделяемый текст при 
%копировании его в буфер обмена
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
%\usepackage{pscyr}  
%\renewcommand{\rmdefault}{ftm} % ftm - (TimesNewRoman), fac - Academy, fad - Advertisement, flz - 
%Lazurski, fcr - CourierNewPSM, others in pscyr.sty

\usepackage{amsthm,amsfonts,amsmath,amssymb,amscd} % Математические дополнения от AMS
\usepackage{mathtools} % Добавляет окружение multlined

\usepackage{longtable} % Длинные таблицы
\usepackage{multirow,makecell,array} % Улучшенное форматирование таблиц
%\usepackage{booktabs} % Возможность оформления таблиц в классическом книжном стиле

\usepackage{soulutf8} % Поддержка переносоустойчивых подчёркиваний и зачёркиваний
\usepackage{icomma} % Запятая в десятичных дробях

\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}

\usepackage{hyperref}

\usepackage{graphicx} % Подключаем пакет работы с графикой
\graphicspath{{../images/}{images/}} % Пути к изображениям

%%% Подписи %%%
\usepackage[singlelinecheck=off,center]{caption}
\usepackage{subcaption}

\usepackage[onehalfspacing]{setspace}

%%% Списки %%%
\usepackage{enumitem}

%%% Библиография %%%
\usepackage{cite} % Красивые ссылки на литературу

%%% Оглавление %%%
\usepackage[nottoc]{tocbibind}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{titlesec} % Растояние между заголовками и текстом
\usepackage{float}
\usepackage{listings} % Listings
\usepackage{minted}

\usepackage{lipsum}

%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\input{styles} % Файл со стилями

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother
%

\begin{document}
\include{cover_page} % Титульник
\include{abstract} % Аннотация
\tableofcontents % Оглавление 
\clearpage

\section{Введение}


Когда в мире происходит какое-либо событие, различные средства массовой информации
пишут статьи с информацией об этом событии в виде новостей. Пользователю часто бывает сложно ориентироваться в большом потоке данных от разных источников. Автоматическая систематизация и обработка таких данных с целью предоставить наиболее полную и информативную картину может сэкономить человеку много времени. 

Человек очень просто понимает информацию, содержащуюся в тексте на естественном языке, потому что он учится этому с рождения, не заметно для себя, выучивая связи между устройством языка и информацией, которую с помощью него передают. С другой стороны, формализация этих правил очень сложна для людей, поэтому на ней сосредоточено множество разделов лингвистики. Сейчас редакторы новостных медиа почти полностью вручную выполняют все задачи, связанные с текстом: размечают теги, собирают подборки и, чаще всего, <<генерируют>> новости полностью опираясь статьи-источники.

Создание математических моделей, описывающих связи между информацией и естественным языком, позволяет автоматизировать эти процессы. Последние десятилетия быстрыми темпами развивается обработка естественного языка (Natural Language Processing, NLP), которая с помощью моделей и алгоритмов решает задачи автоматического анализа текста, в частности, при использовании нескольких источников объединять новостные статьи в группы (кластеры) по релевантности к конкретному событию (кластеризация), извлекать из кластера наиболее информативные данные о событии, например, в виде нескольких предложений. Самые базовые и повседневные интернет-сервисы построены с использованием NLP: поиск, таргетинговая реклама, рекомендательные сервисы и т.п.

{\bf Целью выпускного проекта} является разработка веб-сервиса, который автоматически
группирует русскоязычные новостные статьи по событиям и извлекает из них ключевую информацию в режиме реального времени. 

Для достижения данной цели необходимо решить {\bf следующие задачи}:
\begin{enumerate}
	\item Реализация системы сбора данных: извлечения статей (парсинг) из web-сайтов СМИ для получения новостей вместе с их метаданными.
	\item Изучение алгоритмов и моделей анализа текста: нормализация, векторизация и кластеризация, суммаризация.
	\item Изучение технической реализация модуля анализа текста.
	\item Проектирование и разработка инфраструктуры сервиса, интеграция с ранее реализованными модулями сбора и анализа данных.
	\item Оценка качества сервиса, анализ предложенных решений и выводы.
\end{enumerate}


\section{Данные}
Многие NLP модели, требуют большого количества предварительно обработанных данных для обучения, в частности используемый нами TF-IDF для векторизации текста. В данном случае такими данными является корпус русскоязычных новостей. В исследовательских работах авторы часто используют готовые данные~---общедоступные размеченные датасеты, но если учитывать цель проекта, то без реализации своей системы, позволяющей получать новости с нескольких источников за определённый период времени, не обойтись.

С помощью собственной системы парсинга собран датасет, состоящий из нескольких сотен тысяч новостных статей с сайтов следующих СМИ: <<Новая газета>>, <<Газета.Ru>>, <<Lenta.ru>>, <<ТАСС>>, <<ВЕДОМОСТИ>>, <<Медуза>>, <<РИА Новости>> с метаданными (заголовок, текст, дата, тема). При обработке выяснилось, что у многих статей темы указаны редакторами некорректно (например, у <<РИА Новостей>> большая половина контента помечена тегом <<проишествие>>, у <<Новой газеты>> все новости старее 2014 года~--- тегом <<политика>>). После чистки данных в датасете осталось 130 тыс. статей, имеющих 32 различных тега. Распределение тегов и источников показано на рис. \ref{datadistr}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.35]{datadistr}
	\caption{Распределение тегов и источников в датасете статей для обучения TF-IDF векторизатора.}
	\label{datadistr}
\end{figure}

От объективности данного датасета зависит качество всего сервиса, так как на нём обучается векторизатор TF-IDF, на котором основан алгоритм кластеризации, а от него, в свою очередь, суммаризация события. Для проверки валидности данных и обученного векторизатора реализован классификатор SVM (support vector machine).

SVM методы классификации используют операции линейной алгебры при работе с векторизованым текстом, при обучении <<пытаясь>> разделить многомерное пространство так, чтобы максимальное количество точек  (векторов) одного и того же класса одного класса находилось в одной части пространства. Это можно достичь с помощью перехода к $n+1$-мерному пространству, как условно показано на рис. \ref{svm_cond}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.6]{svm_cond}
	\caption{Явное разделение данных на два класса при переходе в пространство более высокой размерности.}
	\label{svm_cond}
\end{figure}

SVM классификаторы очень популярный инструмент в NLP задачах, поэтому существует множество реализаций с полезными функциями. Мы использовали его модификацию \verb+SGDClassifier+, которая оптимизирует параметры с помощью градиентного спуска.

Проверка валидности состоит в эмпирической оценке признаков классов~--- это самые <<весомые>> слова, больше всего влияющие на принадлежность к конкретному классу. При изучении слов-признаков из таблицы \ref{word_feachers} видно: слова-признаки и слова-классы связаны по смыслу и, в некоторых случаях, являются синонимами, что свидетельствует о корректности данных и векторизатора. Метрики качества классификаторы тоже поддтверждают правильность датасета: $\text{Accuracy} = 0,8687$, $\text{F1 score} = 0,8711$, матрица ошибок представлена в приложении на рис. \ref{svm_matrix}.

\begin{table}[h]
	\caption{Слова-признаки для собранного датасета.}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{r|cccccccc}
			\textbf{animals}         &               жить &          вольер &             хозяин &              животный &               зоопарк &        питомец &           кличка &        животное \\
			\textbf{auto}            &          авторынок &           осаго &      автомобильный &     автопроизводитель &              автопром &          камаз &          автоваз &      автомобиль \\
			\textbf{basketball}      &                рфб &      евробаскет &          центровой &          кубок европа &             баскетбол &   баскетболист &         евролига &             нба \\
			\textbf{biathlon}        &           эстафета &    биатлонистка &            шипулин &                   сбр &            хохфильцен &        биатлон &              ibu &      биатлонист \\
			\textbf{books}           &         библиотека &    произведение &       писательница &                 роман &                 книга &   литературный &             поэт &        писатель \\
			\textbf{boxing}          &              алоян &          лебзяк &                мма &              поединок &              поветкин &            бой &             бокс &          боксер \\
			\textbf{business}        &   россельхознадзор &            fifa &                ржд &               газпром &           туроператор &        formula &         ритейлер &             оао \\
			\textbf{chess}           &            карякин &       шахматист &           карякина &                магнус &               шахматы &        карлсен &              фид &       шахматный \\
			\textbf{companies}       &  тысяча автомобиль &        компания &  миллиард кубометр &              миллиард &                тысяча &  процент акция &         ретейлер &         процент \\
			\textbf{cosmos}          &          космонавт &         светить &           прогресс &             вселенная &                космос &     астрофизик &             марс &       астронавт \\
			\textbf{crime}           &          грабитель &         изымать &        группировка &               полиция &               убивать &         летний &       преступник &          тюрьма \\
			\textbf{cybersport}      &             gaming &            team &              valve &           киберфутбол &                  dota &     киберспорт &  киберспортивный &  киберспортсмен \\
			\textbf{economics}       &               мрот &          греция &             бюджет &                пенсия &                   ввп &         минфин &        экономика &        инфляция \\
			\textbf{films}           &         мультфильм &          сериал &            актриса &                  кино &               картина &       режиссер &            актер &           фильм \\
			\textbf{football}        &               поле &         стадион &         нападающий &              матч тур &                  фифа &           уефа &        футболист &    полузащитник \\
			\textbf{forces}          &       развертывать &      выполнение &     военнослужащий &               военный &                 шойгу &        генштаб &       конашенков &      минобороны \\
			\textbf{formula1}        &              манор &      цитировать &               рено &               феррари &              макларен &          пилот &         мерседес &         формула \\
			\textbf{hockey}          &           авангард &             ска &              шайба &            нападающий &                хоккей &       хоккеист &              нхл &             кхл \\
			\textbf{internet}        &          википедия &          сервис &             ресурс &               youtube &                  сайт &          хакер &           блогер &        интернет \\
			\textbf{judiciary}       &             стража &          статья &       арестовывать &               колония &  следственный комитет &      следствие &              скр &  комитет россия \\
			\textbf{music}           &         композитор &     евровидение &              песня &                 певец &               концерт &         альбом &           певица &        музыкант \\
			\textbf{politics}        &              лидер &          кремль &      парламентарий &                партия &               депутат &        госдума &            глава &             мид \\
			\textbf{realty}          &             объект &    строительный &           жилищный &         строительство &                   жкх &        ипотека &            жилье &    недвижимость \\
			\textbf{religion}        &          монастырь &           собор &          церковный &                муфтий &                святой &       христиан &       митрополит &        патриарх \\
			\textbf{science}         &        университет &          журнал &          математик &                 физик &               научный &       археолог &    исследователь &          ученый \\
			\textbf{skiing}          &             вяльбе &          нортуг &             лыжник &                   fis &                легков &         йохауг &            лахти &         устюгов \\
			\textbf{social-networks} &          некоторые &            юзер &           facebook &               twitter &     пользователь сеть &   пользователь &        вконтакте &         соцсеть \\
			\textbf{technologies}    &          vimpelcom &           apple &           оператор &                  wifi &              говорить &            мтс &            робот &         контакт \\
			\textbf{tennis}          &    кубок федерация &            open &            шарапов &                теннис &                  корт &      теннисист &      теннисистка &     кубок дэвис \\
			\textbf{theatre}         &         росгосцирк &        цирковой &              балет &            постановка &           театральный &         мюзикл &            театр &       спектакль \\
			\textbf{volleyball}      &              факел &         маричев &             алекно &             суперлига &             казанский &      белогорье &      волейболист &        волейбол \\
			\textbf{weapons}         &               jane &  использоваться &       defense news &  министерство оборона &             миллиметр &  миллиметровый &          defense &             тип \\
		\end{tabular}
		}
	\label{word_feachers}
\end{table}


\section{Анализ текста}
\subsection{Нормализация}

Текст на естественном языке содержит много избыточных элементов, без которых его смысл не изменится. Чаще всего они действуют как <<шум>>, так как встречается равномерно по всему корпусу языка. Подобными элементами почти всегда выступают союзы, предлоги, части слов, отвечающие за форму. Кроме того, существуют разные способы написания одних и тех же объектов, например, числительные можно написать цифрами. При решении любой задачи в NLP текст нормализуют, то есть приводят в общую, более информативную форму, без <<шума>>. Нормализация включает в себя несколько шагов.

При работе с естественной информацией её дискретизуют. Похожий процесс в NLP называется токенизация, он заключается в делении текста на части~--- токены, обычно токен является одним словом. К сожалению, просто делить текст по пробелам не совсем корректно, так как существует множество исключений, например, Великие Луки~--- это один токен, хотя и состоит из двух слов. Если рассматривать это как два токена, то смысл текста будет искажён, что может сказаться на результате и на качестве решения задачи. Для токенизации удобно использовать регулярные выражения.

В данном проекте используется два токенизатора: простое деление на слова при обработке текста для TF-IDF и Punkt токенизатор для деления статей на предложения при суммаризации. Последний использует корпус для обучение без учителя, <<выучивая>> последовательности, с которых начинаются предложения, что помогает работать с нелитературными данными, например, сообщениями в соц. сетях, где предложения часто начинаются с маленькой буквы.

Следующий шаг нормализации~--- удаление стоп-слов. Стоп-слова примерно одинаково распределены по всему корпусу языка. В русском языке ими являются многие служебные части (союзы, междометия, предлоги).

Для однозначной идентификации слова его приводят к начальной форме. Данный процесс называется лемматизация, а начальная форма~--- лемма. Для лемматизации недостаточно использовать только словарь, потому что существует огромное количество неологизмов, подчиняющимся тем же морфологическим правилам при образовании форм. Хорошие лемматайзеры проводят полный морфологический парсинг, при котором слова делятся на морфемы: стемы (самые осмысленные части) и афиксы (придают дополнительное значение слову) \cite{c2}. Более простая версия морфологического анализа~--- стемминг, использующий определённые правила для извлечения основы слова.% https://web.stanford.edu/~jurafsky/slp3/2.pdf

Мы используем лемматизатор MyStem, предоставляющий универсальный для многих языков алгоритм морфологического разбора, использующийся в популярном поисковом движке.
%https://cache-mskm910.cdn.yandex.net/download.yandex.ru/company/iseg-las-vegas.pdf

При обработке данных перед векторизаций последовательно выполняются следующие операции:
\begin{enumerate}
	\item приведение текста в нижний регистр;
	\item удаление символов пунктуации;
	\item удаление стоп-слов.
	\item лемматизация каждого слова с помощью Python библиотеки MyStem.
\end{enumerate}
 Примеры предложений после данных действий представлен в таблице. \ref{exa}.


\begin{table}[h!]
	\centering
	\caption{Примеры нормализации текста}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|l|}
			\hline
			Оригинал  & Нормализация \\ \hline\hline
			\makecell[l]{Однако когда их проверили на восприятие\\ концепций и идей, оказалось, что те, \\кто писал от руки, понимают пройденный \\материал лучше однокашников.} & \makecell[l]{однако когда они проверять на восприятие \\концепция идея оказываться что тот \\кто писать от рука понимать проходить\\ материал хорошо однокашник}
			\\ \hline
			\makecell[l]{Лингвистическую относительность упоминали\\ в своих сочинениях немецкие философы\\ еще в конце XVIII – начале XIX века, \\но известность гипотеза получила\\ именно благодаря Уорфу.} & \makecell[l]{лингвистический относительность упоминать \\свой сочинение немецкий философ \\еще конец xviii – начало xix век \\но известность гипотеза получать\\ именно благодаря уорфу} \\ \hline
		\end{tabular}
	}
	\label{exa}
\end{table}


\subsection{Векторизация}

TF-IDF \cite{doi:10.1108/eb026526} --- статистическая мера, используемая для оценки важности слова в любом контексте, основанная на его встречаемости в документе. Чем реже слово появляется в документе, тем выше его TF-IDF вес.

TF-IDF --- это произведение двух статистик: TF (term frequency) и IDF (inverse 
document frequency). 

Существует несколько способов подсчёта TF-IDF, в данной работе использовался следующий:
$$
\text{tf}(t, d) = \cfrac{n_t}{\sum_{k} n_k},
$$
где $n_{t}$ есть число вхождений слова $t$ в документ, а $\sum_{k} n_k$ --- общее число слов в данном документе.

$$
\text{idf}(t, D) = \log{ \cfrac{|D|}{|\{ d_i \in D \mid t \in d_i \}|}},
$$
где $|D|$ --- число документов в корпусе, $|\{ d_i \in D \mid t \in d_i \}|$ — число документов из корпуса $D$, в которых встречается 
$t$ (когда $n_{t} \neq 0$).

Таким образом, мера TF-IDF является произведением двух сомножителей:
$$
\text{TF-IDF}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D).
$$

Признаковым описанием одного объекта $d \in D$ будет вектор
$$
\big(\text{TF-IDF}(t,d,D)\big)_{t\in V},
$$
где $V$ --- словарь всех слов, встречающихся в корпусе $D$.

Обучение модели заключается в подсчёте веса каждого уникального слова в каждом документе.

Одно из полезных свойств векторов TF-IDF: косинусное расстояние между векторами характеризует <<похожесть>> статьей, что можно использовать для кластеризации.

\subsection{Кластеризация}
Самый базовый алгоритм кластеризации, который в данном случае применим~--- алгоритм поиска связных компонентов в графе. Его псевдокод показан на рис. \ref{pseudo}, где $V, E$~--- множество рёбер и вершин графа, $(u,v),(u,n)$~--- рёбра, $v,u,n$~---вершины, 
$w(u,v)$~--- вес ребра $(u,v)$, $c[v]$~--- номер компоненты (кластера) вершины $v$, $Q$~--- очереди вершин для посещения, $S$ множество посещённых вершин, $compNum$~--- номер текущей компоненты.

\begin{figure}[h]
	\begin{algorithm}[H]
		\DontPrintSemicolon
		$ compNum \gets 0 $\;
		\For{$(u,v) \in E$}{
			$w(u,v) \gets \cos{(u,v)}$\;
		}
		\For{$v \in V$}{
			$c[v] \gets  nil $\;
		}
		\For{$v \in V$}{
			\If{$ c[v] = nil $} {
				$ c[v] \gets compNum $\;
				$Q \gets \{v\}$\;
				$S \gets \emptyset$\;
				\While{ $ Q \ne \emptyset$}{
					$u \gets pop(Q)$\;
					$c[u] \gets compNum$\;
					$S \gets S \cup \{v\}$\;
					\For{$n \notin S, (u,n) \in E$}{
						\If{$w(u, n) \geqslant \text{threshold}$ {\bf and} $c[n] = nil $} {
							$Q \gets Q \cup \{n\}$\;
						}
					}
				}
				$ compNum \gets compNum + 1 $
			}
		}
	\end{algorithm}
	\caption{Псевдокод графовой кластеризации}
	\label{pseudo}
\end{figure}

Алгоритм работает следующим образом: строится полный граф, значения ребёр выставляется как косинусное расстояние между векторами-вершинами. Далее, от каждой не помеченной номером кластера вершины запускается BFS алгоритм (breadth-first search, поиск в ширину), игнорирующий рёбра меньше определённого значения, и устанавливающий любой вершине, которой удалось достичь, номер своего кластера. Этот процесс повторяется до тех пор, пока всем вершинам не назначен номер кластера.

Данный алгоритм очень прост в реализации и показывает хорошие результаты, кроме того, ему не нужно заранее знать количество кластеров, единственный параметр threshold~--- порог, ниже которого рёбра игнорируются.


Другой, более популярный, но не менее простой алгоритм кластеризации K-means рассматривает векторы, как точки в пространстве, выбирает $k$ случайных точек-центроидов и все ближайшие к ним точки-вектора статей, формируя $k$ случайных групп  (рис. \ref{km}, б). Далее у каждой группы определяется средняя точка, которая становится новым центроидом (рис. \ref{km}, в). Процесс повторяется до тех пор, пока точки не перестанут сдвигаться (рис. \ref{km}, г--e).

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{k_0}
		\caption{}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{k_1}
		\caption{}
	\end{subfigure}
	~ 
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{k_2}
		\caption{}
	\end{subfigure}
	~ 
	
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{k_3}
		\caption{}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{k_4}
		\caption{}
	\end{subfigure}
	~ 
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{k_5}
		\caption{}
	\end{subfigure}
	\caption{Визуализация кластеризации K-means.}
	\label{km}
\end{figure}

В проекте используется алгоритм K-means, так как при эмпирическом сравнении он показывает результаты чуть лучшие графового подхода и имеет больше параметров, что позволяет его тонко настроить в зависимости от размера данных. Если данных для кластеризации очень много, как это часто бывает, то алгоритму может не хватить оперативной памяти. Для решения этой проблемы существует вариант K-means, принимающий данные по частям (batches), к сожалению, время работы алгоритма при этом увеличивается в зависимости от размера частей.



\subsection{Суммаризация}
Алгоритмы суммаризации можно условно поделить на две группы: абстрактная (abstractive) и извлекающая (extractive). Первая генерирует текст по смыслу, вторая выбирает самые информативные предложения из исходного текста и склеивает их. Модели абстрактной суммаризации требуют больших вычислительных мощностей и сложны в реализации, так как основаны на нейронных сетях, при их использовании стоит учитывать особенности языка и использовать эвристики для генерации корректных форм слов или словосочетаний. Извлекающая суммаризация легче в реализации и выглядит правдоподобно, так как состоит из написанных человеком предложений.

В сервисе используются два алгоритма MDS (multi-document summarization): SumBasic и DivRank, подразумевающие суммаризацию сразу нескольких документов на одну тему, что хорошо подходит для кластеров-событий.
  
\subsubsection{SumBasic}
SumBasic основан на простом наблюдении: слова, появляющиеся часто в кластере документов, с большей вероятностью окажутся в сокращённых текстах, написанных человеком, чем остальные.

Алгоритм SumBasic состоит из следующих шагов:
\begin{enumerate}
	\item Для каждого уникального слова $w_i$ посчитать вероятность его присутствия во входных данных $p(w_i)=\frac{n}{N}$, где $n$~--- количество вхождений слова $w_i$ в данных, а $N$~--- общее количество слов.
	\item Взвесить каждое предложение $S_j$, посчитав среднюю вероятность $p(w_i)$:
	$$ weight(S_j) = \sum_{w_i\in S_j} \frac{p(w_i)}{|\{w_i|w_i\in S_j\}|}$$
	\item Выбрать предложение с лучшим весом, содержащие слово с максимальной вероятностью.
	\item Для каждого слова $w_i$ в выбранном предложении обновить их вероятность:
	$$p_{new}(w_i) = p_{old}(w_i)^2$$
	\item Если уже выбранных предложений недостаточно перейти на шаг 2. 
\end{enumerate} 

Шаг 3 гарантирует, что предложение с самым вероятным словом будет выбрано. Шаг 4 добавляет алгоритму <<чувствительность>> к контексту сокращения: обновляя вероятности таким образом мы позволяем изначально невероятным словам оказывать большее влияние на выбор предложений, и, самое главное, этот шаг позволяет эффективно применять алгоритм для нескольких документов, позволяя игнорировать уже сокращённую информацию, реализуя <<затухание>> вероятности слов.

SumBasic выбран из-за его простоты и эллегантности для сравнения с более сложным алгоритмом DivRank.
\subsubsection{DivRank}

DivRank (Diverse Rank)~--- алгоритм для взвешивания графов, подобный популярному PageRank, но применяющийся для MDS суммаризации. Для его использования необходимо построить граф, схожий с тем, что описан в графовом методе кластеризации в п.3.3. Различия в том, что здесь вершины~--- это TF-IDF вектора предложений, а не статей. Рёбра удаляются, если косинусное расстояние меньше 0.1. После взвешивания этого графа DivRank'ком выбираются $k$ первых самых весомых предложений.

На рис. \ref{div}, в показан <<разнообразно>> взвешенный граф с помощью DivRank в сравнении с графом, взвешенный используя PageRank (рис. \ref{div}, б). Если необходимо выбрать 3 вершины, максимально и ёмко передающие информацию о графе, то алгоритм DivRank выдаст вершины 1,4,5, что даже визуально лучше, чем ответ алгоритма 1,2,3 PageRank. 
\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{NoR}
		\caption{Исходный граф}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{PageR}
		\caption{PageRank}
	\end{subfigure}
	~ 
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{DivR}
		\caption{DivRank}
	\end{subfigure}
	\caption{Взвешивание графа}
	\label{div}
\end{figure}


\section{Реализация сервиса}
\subsection{Архитектура и транспорт данных}
\subsection{Особенности реализации компонентов анализа}
\subsubsection{Система сбора данных}

Система сбора данных представляет собой коллекцию парсеров сайтов российских СМИ, основанные на одном подходе и имеющие одинаковый интерфейс.

Чтобы понять, как реализовать универсальную систему, с возможностью быстрого добавления поддержки нового ресурса, достаточно взглянуть на сайты русскоязычных медиа-ресурсов. Многие сильно отличаются друг от друга внешне, но у всех присутствует следующая логика: существуют страницы со списком новостей в хронологическом порядке, которые либо агрегированы по дням (Lenta.ru, Gazeta.ru, vedomosti.ru), либо используют параметр offset, указывающий с какой статьи начинать страницу (novayagazeta.ru, tass.ru, meduza.io). Первый вариант пагинации удобнее, потому что позволяет просто получить новости за любой заданный интервал времени. Во втором случае можно использовать бинарный поиск по страницам, но это не реализовано, так как новости всегда нужны с текущего момента.
Большинство СМИ отдают данные в HTML формате и только лишь малая часть использует API в JSON формате.

При инициализации сервиса необходимо получить новости за последние несколько часов, чтобы сформировать актуальные кластера, что занимает долгое время. Парсинг множества статей можно ускорить в несколько раз, обрабатывая их параллельно.

Так как система обособлена, то реализовывать её можно на любом языке, а взаимодействовать с сервисом через внешнее хранилище, но для удобства разработки выбран \texttt{Python 3}, с использованием дополнительных библиотек: 
\begin{itemize}
	\item \texttt{BeautifulSoup}~--- парсинг HTML.
	\item \texttt{multiprocessing}~--- распараллеливание задач.
	\item \texttt{pymongo}~--- интерфейс над внешним хранилищем Mongodb.
\end{itemize}

Так как Питон имеет ограничение на потоки из-за GIL, то чтобы обеспечить параллелизм, позволяющий с увеличением количества процессорных ядер ускорять обработку множества статей, используется модуль \texttt{multiprocessing}, что значительно усложняет задачу. Использование процессов вместо корутин и асинхронных задач в данном случае не даст большой прирост производительности, так как большинство процессорного времени тратиться не на чтение сокетов, а на парсинг HTML. 

Пример использования представлен на рис. \ref{example}. 

\begin{figure}
	\centering
	\begin{minted}[fontsize=\small]{python}
	from parsers import Gazeta, Tass, Lenta, Vedomosti, Novaya, Meduza,
	    process_init
	import datetime
	from pymongo import MongoClient
	from multiprocessing import Pool, cpu_count
	mongo_client = MongoClient('localhost', 27017)
	
	pool = Pool(processes=cpu_count, initializer=process_init)
	parsers_ = [
	    Gazeta(), Tass(), Meduza(), Lenta(), Vedomosti(), Novaya()
	]
	until = datetime.datetime.now() - datetime.timedelta(hours=4)
	for parser in parsers_:
	    parser.parse(pool, until_time=until)
	pool.close()
	pool.join()
	news = list(mongo_client.news.raw_news.find({}))
	\end{minted}
	\caption{Пример использования для получения всех новостей за последние 4 часа.}
	\label{example}
\end{figure}

Главной частью системы является класс \texttt{BaseParser}, инкапсулирующий сетевые запросы, работу по синхронизации процессов и передаче данных между ними. В целом, логика межпроцессного взаимодействия системы достаточно тривиальная и находится в методе \texttt{parse} (рис. \ref{parse}): в метод передаётся объект pool, принимающий задачи парсинга статей, задачи создаются во время обработки страницы с лентой новостей сайта. Задачи пишут ответы в базу данных mongodb, откуда их потом извлекают другие контейнеры при необходимости. 

\begin{figure}
	\centering
	\begin{minted}[fontsize=\small]{python}

	. . .
url_to_fetch = self._page_url()
while True:
	content = self._get_content(url_to_fetch, type_=self.page_type)
	news_list = self._get_news_list(content)
    for news in news_list:
        try:
            # Url always first, timestamp always second in params
            news_params = self._get_news_params_in_page(news)
            self.curr_date = news_params[1]
            if (self.curr_date <= until_time):
                break
            # Pushing task to pool queue
            pool.map_async(self._process_news, [(news_params)])
    else:
        url_to_fetch = self._next_page_url()
        continue
    break
. . .
	\end{minted}
	\caption{Часть метода \texttt{BaseParser.parse} с основной логикой.}
	\label{parse}
\end{figure}


Чтобы создать новый парсер необходимо наследоваться от \texttt{BaseParser}, вызвать родительский конструктор с параметрами: название парсера, URL-префикс любой новости ресурса, URL-префикс ленты новостей, дефолтное количество процессов. Определить следующие методы:
\begin{itemize}
	\item \texttt{\_get\_news\_list(self, content)}~--- возвращает список списков с необработанными параметрами новости, используя контент (HTML или JSON) ленты новостей.
	\item \texttt{\_get\_news\_params\_in\_page(self, news)}~--- возвращает \texttt{tuple} с параметрами статьи, используя элемент списка из предыдущего метода. URL и дата (в формате \texttt{timestamp}) должны быть первыми в списке параметров (\texttt{dict} не используется в данном случае в попытке выиграть время и место на pickle-запаковке объекта).
	\item \texttt{\_parse\_news(self, news\_params)}~--- возвращает \texttt{dict} с новостью, например, \mintinline[fontsize=\small]{python}/{'title': title, 'url': url, 'text': text, 'topic': topic, 'date': date}/.
	\item \texttt{\_page\_url(self)}~--- возвращает URL текущей страницы.
	\item \texttt{\_next\_page\_url(self)}~--- <<переворачивает>> страницу и возвращает \texttt{\_page\_url}.
\end{itemize}





\begin{figure}
	\centering
	\begin{minted}[fontsize=\small]{python}
	from nltk.corpus import stopwords
	from pymystem3 import Mystem; mystem = Mystem()
	import string
	from stop_words import get_stop_words
	
	STOP_WORDS = (set(stopwords.words('russian')) | set(stopwords.words('english')) | 
	set(get_stop_words('ru')) | set(get_stop_words('en')))
	
	def get_word_normal_form(word):
	return ''.join(mystem.lemmatize(word)).strip().replace('ё', 'е').strip('-')
	
	def lemmatize_words(text):
	text = text.lower()
	text = ''.join([i for i in text if ( i not in string.punctuation )])
	res = []
	for word in text.split():
	norm_form = get_word_normal_form(word)
	if len(norm_form) > 2 and norm_form not in STOP_WORDS:
	res.append(norm_form)
	return ' '.join(res)
	\end{minted}
	\caption{Простая функция нормализации.}
	\label{norm}
\end{figure}




\subsubsection{Модуль нормализации}
\subsubsection{TF-IDF и SVM}
\subsubsection{K-means}
\subsubsection{SumBasic}
\subsubsection{DivRank}

\subsection{Инфраструктура и развёртка}

\section{Оценка решений}
\section{Заключение}


\bibliography{biblio} % Список литературы

\section*{Приложение}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.95\textwidth]{svm_matrix}
	\caption{Ошибки классификатора SVM}
	\label{svm_matrix}
\end{figure}

\end{document}