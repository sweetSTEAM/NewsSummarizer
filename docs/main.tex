\documentclass[a4paper, 14pt]{extarticle}
\usepackage{geometry}

\usepackage{cmap} % Улучшенный поиск русских слов в полученном pdf-файле
\usepackage{mathtext} % русские буквы в формулах
\defaulthyphenchar=127 % Если стоит до fontenc, то переносы не впишутся в выделяемый текст при 
%копировании его в буфер обмена
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
%\usepackage{pscyr}  
%\renewcommand{\rmdefault}{ftm} % ftm - (TimesNewRoman), fac - Academy, fad - Advertisement, flz - 
%Lazurski, fcr - CourierNewPSM, others in pscyr.sty

\usepackage{amsthm,amsfonts,amsmath,amssymb,amscd} % Математические дополнения от AMS
\usepackage{mathtools} % Добавляет окружение multlined

\usepackage{longtable} % Длинные таблицы
\usepackage{multirow,makecell,array} % Улучшенное форматирование таблиц
%\usepackage{booktabs} % Возможность оформления таблиц в классическом книжном стиле

\usepackage{soulutf8} % Поддержка переносоустойчивых подчёркиваний и зачёркиваний
\usepackage{icomma} % Запятая в десятичных дробях

\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}

\usepackage{hyperref}

\usepackage{graphicx} % Подключаем пакет работы с графикой
\graphicspath{{../images/}{images/}} % Пути к изображениям

%%% Подписи %%%
\usepackage[singlelinecheck=off,center]{caption}
\usepackage{subcaption}

\usepackage[onehalfspacing]{setspace}

%%% Списки %%%
\usepackage{enumitem}

%%% Библиография %%%
\usepackage{cite} % Красивые ссылки на литературу

%%% Оглавление %%%
\usepackage[nottoc]{tocbibind}
\usepackage{tocloft}
\usepackage{titlesec}

\usepackage{titlesec} % Растояние между заголовками и текстом
\usepackage{float}
\usepackage{listings} % Listings
\usepackage{minted}

\usepackage{lipsum}

%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\input{styles} % Файл со стилями

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother
%

\begin{document}
\include{cover_page} % Титульник
\include{abstract} % Аннотация
\tableofcontents % Оглавление 
\clearpage

\section{Введение}

Когда в мире происходит какое-либо событие, различные средства массовой информации
пишут статьи с информацией об этом событии в виде новостей. Пользователю часто бывает сложно ориентироваться в большом потоке данных от разных источников. Автоматическая систематизация и обработка таких данных с целью предоставить наиболее полную и информативную картину может сэкономить человеку много времени. 

Модели и алгоритмы обработки естественного языка (Natural Language Processing, NLP) решают задачи автоматического анализа текста и позволяет: при использовании нескольких источников объединять новостные статьи в группы (кластеры) по релевантности к конкретному событию (кластеризация), извлекать из кластера наиболее информативные данные о событии, например, в виде нескольких предложений.

Основной целью выпускного проекта является разработка веб-сервиса, который автоматически
группирует российские новостные статьи по событиям и извлекает из них ключевую информацию в режиме реального времени. 

Для достижения данной цели необходимо решить следующие задачи:
\begin{enumerate}
	\item Реализация системы сбора данных: парсинг сайтов СМИ для получения новостей вместе с их метаданными.
	\item Изучение алгоритмов и моделей анализа текста: нормализация, векторизация и кластеризация, суммаризация.
	\item Техническая реализация модуля анализа текста.
	\item Проектирование и разработка инфраструктуры сервиса, интеграция с ранее реализованными модулями сбора и анализа данных.
	\item Оценка качества сервиса, выводы о проделанной работе.
\end{enumerate}

Достижение цели проекта и успешное выполнение задач определяется получением следующих результатов:
\begin{enumerate}
	\item Комбинация моделей NLP, обученных на русском языке, корпусе и
	алгоритмах, подходящих для кластеризации и сокращения новостных статей.
	\item Веб-сервис, который демонстрирует работу предлагаемых решений для
	эмпирической оценки качества методов обобщения и кластеризации.
\end{enumerate}


\section{Система сбора данных}
{\LARGE ПЕРЕПИСАТЬ, ВСЁ УЖЕ НЕ ТАК}
Многие модели, используемые в решениях NLP задач, требуют большого количества предварительно обработанных данных для обучения, в данном случае такими данными является корпус русскоязычных новостей. В исследовательских работах авторы часто используют готовые данные~---общедоступные размеченные датасеты, но если учитывать цель проекта, то без реализации своей системы, позволяющей получать новости с нескольких источников за определённый период времени, не обойтись.

Система сбора данных представляет собой коллекцию парсеров сайтов российских СМИ, основанные на одном подходе и имеющие одинаковый интерфейс. Пример использования представлен на рис. \ref{example}.

В общей архитектуре сервиса система является обособленным фоновым процессом, собирающим раз в некоторое время новые статьи, поэтому нет необходимости постоянно поддерживать строгие ограничения к скорости парсинга. Но эту же систему можно использовать для относительно быстрого сбора собственного датасета. Кроме того, при инициализации сервиса понадобится получить новости за последние несколько часов, чтобы сформировать актуальные кластера. Парсинг множества статей можно ускорить в несколько раз, обрабатывая их параллельно.

Чтобы понять, как реализовать универсальную систему, с возможностью быстрого добавления поддержки нового ресурса, достаточно взглянуть на сайты русскоязычных медиа-ресурсов. Многие сильно отличаются друг от друга внешне, но у всех присутствует следующая логика: существуют страницы со списком новостей в хронологическом порядке, которые либо агрегированы по дням (Lenta.ru, Gazeta.ru, vedomosti.ru), либо используют параметр offset, указывающий с какой статьи начинать страницу (novayagazeta.ru, tass.ru, meduza.io). Первый вариант пагинации удобнее, потому что позволяет просто получить новости за любой заданный интервал времени. Во втором случае можно использовать бинарный поиск по страницам, но это не реализовано, так как новости всегда нужны с текущего момента.
Большинство СМИ отдают данные в HTML формате и только лишь малая часть использует API в JSON формате.

Так как система обособлена, то писать её можно на любом языке, а взаимодействовать с сервисом через внешнее хранилище, но для удобства выбран \texttt{Python 3}, с использованием дополнительных библиотек: \texttt{BeautifulSoup} для парсинга HTML и \texttt{requests} для выполнения HTTP запросов. Так как Питон имеет ограничение на потоки из-за GIL, то чтобы обеспечить параллелизм, позволяющий с увеличением количества процессорных ядер ускорять обработку множества статей, используется модуль \texttt{multiprocessing}.

\begin{figure}
	\centering
	\begin{minted}[fontsize=\small]{python}
	from parsers import Gazeta, Tass, Lenta, Vedomosti, Novaya
	import datetime
	
	parsers = [
	Gazeta(procs=4), # Number of processes used
	Tass(),
	Lenta(),
	Vedomosti(),
	Novaya(procs=4)
	]
	until_time = datetime.datetime.now() - datetime.timedelta(hours=4)
	for parser in parsers:
	print(parser.id)
	for n in parser.get_news(until_time=until_time):
	print(n['title'])
	\end{minted}
	\caption{Пример использования для получения всех новостей за последние 4 часа.}
	\label{example}
\end{figure}

Главной частью системы является класс \texttt{BaseParser}, инкапсулирующий сетевые запросы, работу по синхронизации процессов и передаче данных между ними. В целом, логика межпроцессного взаимодействия системы достаточно тривиальная и находится в методе \texttt{get\_news} (рис. \ref{getnews}): процесс-предок запускает процессы-потомки и начинает заполнять очередь задач ссылками на статьи, в этот момент дочерние процессы уже разбирают ссылки из очереди и обрабатывают их. В качестве очереди задач выступает класс \texttt{multiprocessing.Queue}, который комбинирует межпроцессное взаимодействие через pipe и разделяемые блокировки. Благодаря модулю pickle очередь может передавать сложные объекты и часто применяется в подобных случаях. 

После того, как основной процесс закончил парсинг страниц и отправил все задачи в очередь, он меняет значение переменной \texttt{sync\_flag}, которая находится в общем для процессов сегменте памяти (Shared memory), если значение меняется, процессы-потомки понимают, что после опустошения очереди можно больше её не <<слушать>>.

\begin{figure}
	\centering
	\begin{minted}[fontsize=\small]{python}
	def get_news(self, start_time=None, until_time=None,
	news_count=None, topic_filter=None):
	. . .
	
	Q_urls = Queue(0) # News urls and for deeper parsing 
	Q_out = Queue(0) # Results of parsing
	sync_flag = Value('i', 1) # Flag to stop processes
	
	workers = []
	# Getting news by url in proceses
	for _ in range(procs):
	workers.append(Process(target=self._process_news,
	args=(Q_urls, Q_out, sync_flag, topic_filter)))
	workers[-1].start()
	# Parsing pages with urls ("Лента новостей") and putting them to Q_urls
	self.parse_pages(Q_urls, sync_flag, start_time, 
	until_time, news_count, topic_filter)
	# Clearing output queue while processes still working
	# Probably significantly slowing down other workers, need to fix it
	out = []
	self._listen_queue(workers, Q_out, out)
	. . .
	\end{minted}
	\caption{Часть кода класса \texttt{BaseParser}.}
	\label{getnews}
\end{figure}

Результаты выполненных задач не принято передавать родительскому процессу, но в данном случае это было сделано для удобства интерфейса с помощью второй очереди. После смены значения общего флага предок начинает <<слушать>> очередь и ждать результатов. Такой подход крайне нежелателен, так как результат в разы больше параметров самой задачи (из-за текста статьи), и при большом количестве обработанных новостей все процессы-потомки останутся висеть в простое, пока предок будет извлекать из очереди результаты. На небольших объёмах это незаметно, но запускать такое на 64 ядерном процессоре в 64 процесса с целью выкачать новости за несколько лет не стоит.

Описанная проблема будет решена заменой Pipe-очереди на базу данных в оперативной памяти (in-memory database), например, на Redis. Можно решить её и с использованием Shared memory, но это гораздо сложнее, так как объекты Питона придётся сериализовывать для хранения и десериализовывать для использования вручную.

Чтобы создать новый парсер необходимо наследоваться от \texttt{BaseParser}, вызвать родительский конструктор с параметрами: название парсера, URL-префикс любой новости ресурса, URL-префикс ленты новостей, дефолтное количество процессов. Определить следующие методы:
\begin{itemize}
	\item \texttt{\_get\_news\_list(self, content)}~--- возвращает список списков с необработанными параметрами новости, используя контент (HTML или JSON) ленты новостей.
	\item \texttt{\_get\_news\_params\_in\_page(self, news)}~--- возвращает \texttt{tuple} с параметрами статьи, используя элемент списка из предыдущего метода. URL и дата (в \texttt{datetime} объекте) должны быть первыми в списке параметров (\texttt{dict} не используется в данном случае в попытке выиграть время и место на pickle-запаковке объекта).
	\item \texttt{\_parse\_news(self, news\_params)}~--- возвращает \texttt{dict} с новостью, например, \mintinline[fontsize=\small]{python}/{'title': title, 'url': url, 'text': text, 'topic': topic, 'date': date}/.
	\item \texttt{\_page\_url(self)}~--- возвращает URL текущей страницы.
	\item \texttt{\_next\_page\_url(self)}~--- <<переворачивает>> страницу и возвращает \texttt{\_page\_url}.
\end{itemize}


\section{Анализ текста}
\subsection{Нормализация}
В данной работе рассматриваются и используются решения следующих NLP задач: нормализация, векторизация, кластеризация, суммаризация.

Текст на естественном языке содержит много избыточных элементов, без которых его смысл не изменится, но которые влияет на точность моделей, так как могут быть непостоянными и действовать как шум. Поэтому при решении любой задачи в NLP текст нормализуют, то есть приводят в общую, более удобную форму. Нормализация включает в себя несколько шагов.

Чтобы с информацией можно было работать, её дискретизуют. Похожий процесс в NLP называется токенизация, заключающийся в делении текста на части~--- токены, чаще всего токен является одним словом. К сожалению, нельзя просто делить текст по пробелам, так как существуют множество исключений, например, Великие Луки~--- это один токен, хотя и состоит из двух слов. Если рассматривать это как два токена, то смысл текста будет искажён, что может сказаться на результате и на качестве решения задачи. Для токенизации удобно использовать регулярные выражения.

Следующий шаг нормализации~--- удаление стоп-слов. Стоп-слова примерно одинаково распределены по всему корпусу языка. В русском языке многие служебные части (союзы, междометия, предлоги и т.д.) являются стоп-словами.

Для однозначной идентификации слова его приводят к начальной форме. Данный процесс называется лемматизация, а начальная форма~--- лемма. Для лемматизации недостаточно использовать только словарь, потому что существует огромное количество неологизмов, подчиняющимся тем же морфологическим правилам при образовании форм. Хорошие лемматайзеры проводят полный морфологический парсинг, при котором слова делятся на морфемы: стемы (самые осмысленные части) и афиксы (придают дополнительное значение слову) \cite{c2}. Более простая версия морфологического анализа~--- стемминг, использующий определённые правила для извлечения основы слова.

% https://web.stanford.edu/~jurafsky/slp3/2.pdf

Простая функция нормализации показана на рис. \ref{norm}. В функции последовательно применяются следующие операции:
\begin{enumerate}
	\item приведение текста в нижний регистр;
	\item удаление символов пунктуации;
	\item удаление стоп-слов.
	\item лемматизация каждого слова с помощью Python библиотеки MyStem.
\end{enumerate}
% Примеры работы данной функции на рис. \ref{exa}.


\begin{figure}
	\centering
	\begin{minted}[fontsize=\small]{python}
	from nltk.corpus import stopwords
	from pymystem3 import Mystem; mystem = Mystem()
	import string
	from stop_words import get_stop_words
	
	STOP_WORDS = (set(stopwords.words('russian')) | set(stopwords.words('english')) | 
	set(get_stop_words('ru')) | set(get_stop_words('en')))
	
	def get_word_normal_form(word):
	return ''.join(mystem.lemmatize(word)).strip().replace('ё', 'е').strip('-')
	
	def lemmatize_words(text):
	text = text.lower()
	text = ''.join([i for i in text if ( i not in string.punctuation )])
	res = []
	for word in text.split():
	norm_form = get_word_normal_form(word)
	if len(norm_form) > 2 and norm_form not in STOP_WORDS:
	res.append(norm_form)
	return ' '.join(res)
	\end{minted}
	\caption{Простая функция нормализации.}
	\label{norm}
\end{figure}


\begin{figure}
	\centering
	{ %\small
		\begin{tabular}{|c|c|}
			\hline
			Оригинал  & Нормализация \\ \hline\hline
			\makecell{Однако когда их проверили\\ на восприятие концепций и идей,\\ оказалось, что те, кто писал от руки,\\ понимают пройденный \\материал лучше однокашников.} & \makecell{однако когда они проверять\\ на восприятие концепция идея\\ оказываться что тот кто писать\\ от рука понимать проходить\\ материал хорошо однокашник}
			\\ \hline
			\makecell{Лингвистическую относительность упоминали\\ в своих сочинениях немецкие философы\\ еще в конце XVIII – начале XIX века, \\но известность гипотеза получила\\ именно благодаря Уорфу.} & \makecell{лингвистический относительность упоминать \\свой сочинение немецкий философ \\еще конец xviii – начало xix век \\но известность гипотеза получать\\ именно благодаря уорфу} \\ \hline
		\end{tabular}
	}
	\caption{Примеры нормализованного текста}
	\label{exa}
\end{figure}


\subsection{Векторизация}

TF-IDF \cite{doi:10.1108/eb026526} --- статистическая мера, используемая для оценки важности слова в контексте 
документа, являющегося частью коллекции документов или корпуса.
TF-IDF --- это произведение двух статистик: TF (term frequency) и IDF (inverse 
document frequency). На сегодняшний день, TF-IDF один из самых популярных способов взвешивания слов, входящих в корпус документов.
Например, 83\% рекомендательных систем цифровых библиотек используют TF-IDF \cite{Beel2016}.

Существует множество способов подсчёта TF-IDF, в данной работе использовался следующий:
$$
\text{tf}(t, d) = \cfrac{n_t}{\sum_{k} n_k},
$$
где $n_{t}$ есть число вхождений слова $t$ в документ, а $\sum_{k} n_k$ --- общее число слов в данном документе.

$$
\text{idf}(t, D) = \log{ \cfrac{|D|}{|\{ d_i \in D \mid t \in d_i \}|}},
$$
где $|D|$ --- число документов в корпусе, $|\{ d_i \in D \mid t \in d_i \}|$ — число документов из коллекции $D$, в которых встречается 
$t$ (когда $n_{t} \neq 0$).

Таким образом, мера TF-IDF является произведением двух сомножителей:
$$
\text{TF-IDF}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D).
$$

Признаковым описанием одного объекта $d \in D$ будет вектор
$$
\big(\text{TF-IDF}(t,d,D)\big)_{t\in V},
$$
где $V$ --- словарь всех слов, встречающихся в коллекции $D$.


\subsection{Кластеризация}
Представленные в виде векторов статьи можно кластеризовать алгоритмом k-means. Если подобрать оптимальные параметры количества кластеров и максимального расстояния от центра кластера, после которого статья удаляется из кластера, то статьи в одном кластере будут об одном событии \cite{km}.

\subsection{Суммаризация}
\subsubsection{SumBasic}
\subsubsection{DivRank}


\section{Реализация сервиса}
\subsection{Архитектура}
\subsection{Инфраструктура и развёртка}

\section{Заключение}


\bibliography{biblio} % Список литературы

\end{document}